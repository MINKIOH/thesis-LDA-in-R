\chapter{Appendix: Web Scraping a Corpus of PNAS Journal Abstracts}
\label{ch:web-scraping}

In order to reproduce and further expand upon the article ``Finding scientific topics'' \citep{Griffiths2004} I had to download the corpus of \acronym{Pnas} abstracts and the corresponding metadata that were used. The general process is called \textbf{web scraping} or \textbf{screen scraping}, which is automated extraction of data from web pages. In this appendix I describe how the \acronym{Pnas} web site is structured and how I implemented the web scraping in the \proglang{Python} scripting language. Furthermore I show how the \acronym{Pnas} corpus that is subject to various analyses in this thesis was scraped and produced. All source code listings can be found in the final section of this appendix.

\section{Site Structure and Content}
The \acronym{Pnas} archive can be accessed via the \acronym{Url} \url{http://www.pnas.org/content/by/year}. All issues for a given year are listed one level deeper, for example \url{http://www.pnas.org/content/by/year/1991}. \acronym{Pnas} is published bi-weekly, in practice this equals approximately 24 to 26 issues per year. The table of content of a single issue can be accessed by \acronym{Url}s like: \url{http://www.pnas.org/content/88/15.toc}, which translates into volume~88 (=year 1991), issue 15. This page level finally lists all the issue's research articles, commentaries, retractions, corrections, etc. and the respective links to abstracts (in \acronym{Html} format, from 1969 onwards) and full texts (in both \acronym{Pdf} and \acronym{Html} format, the latter only from 1996 onwards). 
From 1996's issue 23 onwards, this page is also divided into category sections, which I used to directly extract the major and minor categories (for a distinction between major and minor categories see Chapter~\ref{sect:classification-2001}) needed for the 2001-specific analysis.
Where no categories are provided I also downloaded the full text in \acronym{Pdf} format which contains a machine-readable category description in the page header.

\section{Implementation}

The following programs were used, all being part of official repositories of Debian~6.0:
\begin{itemize}
 \item \textbf{Python} 2.6.6 -- a high level scripting language (\proglang{Python} 2.5.5 was also tested successfully). See for example \cite{Ascher2005}.
 \item \textbf{Scrapy} 0.10.3 -- a web scraping framework implemented in \proglang{Python}.
 \item \textbf{pycurl} 7.19.0-3+b1 -- \proglang{Python} bindings for \pkg{libcurl}, a multi-protocol file transfer library.
 \item \textbf{BeautifulSoup} 3.1.0.1-2 -- an \acronym{Html} parser for \proglang{Python}.
 \item \textbf{pdftotext} 0.12.4 -- part of the \pkg{poppler} \acronym{Pdf} utility package.
 \item \textbf{OpenSSH} 5.5p1 -- the Secure shell, for proxy-tunnelling downloads through a remote host \citep{Barrett2001} (optional).
 \item \textbf{Gnumeric} -- this spreadsheed is useful for manual checks of \acronym{Csv} files (optional).
\end{itemize}

\textbf{Note:} All script run times reported by me in later subsections were measured on Debian~6.0 running on a workstation equipped with an Intel E8400 Core 2 Duo processor and 4 GiB working memory. It is highly likely that the scripts will perform equally fast on other systems because their main bottleneck is file input and output.

\subsection{Preparing Scrapy}
A project folder and default settings can be generated by calling:
\begin{verbatim}
scrapy startproject pnas
\end{verbatim}
At this point one would begin adapting the default scripts and settings to a specific webpage. \pkg{Scrapy} provides access to \acronym{Html} data via \acronym{Xml} Path Language (XPath) selectors, which allows add-ons to the \pkg{Mozilla Firefox} browser like \pkg{XPather} or \pkg{Firebug} to be used for determining the location of data containers in an \acronym{Html} document.

For our purpose I had to modify the default \pkg{Scrapy} files \texttt{settings.py} (listing: Section~\ref{sec:settings.py}), \texttt{items.py} (Section~\ref{sec:items.py}), \texttt{pnas-spider.py} (Section~\ref{sec:pnas-spider.py}) and \texttt{pipelines.py} (Section~\ref{sec:pipeline.py})

A test of how the crawler performs can be made by calling:
\begin{verbatim}
scrapy shell http://www.pnas.org/content/by/year/1990
\end{verbatim}
for the list of issues per year, or, for an issue's table of contents:
\begin{verbatim}
scrapy shell http://www.pnas.org/content/98/3.toc
\end{verbatim}

\subsection{Calling Scrapy}
Once \pkg{Scrapy} is set up, the download of the corpus from 1991 to 2001 can begin:
\begin{verbatim}
scrapy crawl pnas --set FEED_FORMAT=csv --set FEED_URI=scraped.csv \
    --set DOWNLOAD_DELAY=10
\end{verbatim}
As indicated by the arguments to the call, the scraped items will be saved in a \acronym{Csv} (comma separated values) file, which can be imported into most spreadsheets or databases. A download delay of ten seconds was specified to avoid getting blocked by the \acronym{Pnas} web site. This value was found by trial and error.

Depending on the current server load, the crawler will finish in approximately 30~minutes and produce a \acronym{Csv} file with approximately 30,000~rows, each row representing an entry in an issue's table of content. A row consists of fields such as category, authors, title, year and \acronym{Url}s to abstracts and full text versions.

\subsection{Selecting All Further Downloads}
\label{sec:selecting}
The next \proglang{Python} script \texttt{2-select.py }(listing: see Section~\ref{sec:2-select.py}) reads all items from \texttt{scraped.csv} and decides which data is relevant for the corpus and its metadata.
A new file called \texttt{selected.csv} is written which extends \texttt{scraped.csv} by three more fields. These fields specify whether an item should be downloaded as abstract and full \acronym{Pdf}, and whether an item appears more than once (optional, not needed at this step because duplicates are simply overwritten).

\texttt{2-select.py} can be called with an additional switch (\texttt{-m}) to download only missing files, i.e. abstracts and \acronym{Pdf}s which are not found in the local folder structure.

\textbf{Note:} All \proglang{Python} scripts (except for \texttt{4-scrub.py}) depend on a module called \texttt{csv\_unicode.py}, which provides a \acronym{Csv} reader for \acronym{Utf-8} (Unicode) format (listing: see Section~\ref{sec:csv_unicode.py}).

\subsection{Downloading the Selected Abstracts and Full Texts}

\texttt{3-get.py} (listing in Section~\ref{sec:3-get.py}) reads \texttt{selected.csv} and downloads -- depending on the supplied arguments -- abstracts in \acronym{Html} format (\texttt{abstracts}) and/or full \acronym{Pdf}s (\texttt{fullpdfs}).
Downloading all abstracts takes nearly 120 hours, all full \acronym{Pdf}s another 60 hours. Again, this is caused by a necessary interval of ten seconds between downloads.
In order to cut total the time by half by downloading from two different \acronym{Ip}~addresses, I make use of a \acronym{Ssh} tunnel to a user account on a remote server.
A local Socks5 web proxy is set up by calling \texttt{ssh}:
\begin{verbatim}
ssh -D 8080 -v -N h0053049@login.wu.ac.at
\end{verbatim}
In one shell the script is executed by calling:
\begin{verbatim}
./3-get.py abstracts
\end{verbatim}
for downloading all abstracts. In another shell, this time for downloading all \acronym{Pdf}s, the script is called with different arguments:
\begin{verbatim}
./3-get.py fullpdfs socks
\end{verbatim}
Whenever the script (called with the \texttt{socks} argument) makes requests to the local port~8080, they are forwarded to the remote server and thus appear to the \acronym{Pnas} web site as coming from a different location, thereby circumventing the download limit of one request per 10~seconds.

Failed downloads caused by server timeouts are caught by the script and fed into a download loop which starts after all regular downloads end. This loop then terminates either successfully (by finishing all downloads) or by being interrupted via keyboard (Ctrl-C), in which case the missing files' names are stored in a text file (for manual download).

Downloaded files are stored in the current working directory in folders by year. Filenames are similar to the download \acronym{Url}s, with the page number first (page numbering is continuous for each volume, i.e. year) and the extension is either \texttt{.abstract} for \acronym{Html} abstracts or \texttt{.full.pdf} for full text \acronym{Pdf}s. Example: \texttt{1991/8831.full.pdf}.

% http://superuser.com/questions/87776/show-file-size-by-type-in-linux
% find -name '*.pdf' -exec ls -l {} \; | awk '{ SUM += $5} END { print SUM/1024/1024 }'
% find -name '*.abstract' -exec ls -l {} \; | awk '{ SUM += $5} END { print SUM/1024/1024 }'
% find -name '*.pdf' | wc -l
% ./2-select.py -m -d -s

The total size of all relevant 27,664~\texttt{.abstract} files is approximately 1 GiB, all 14,681 \texttt{.full.pdf} files together amount to 19~GiB.

\subsection{Cleaning the Downloaded Files}
Further processing of the downloaded files is handled by the script \texttt{4-scrub.py} (listing in Section~\ref{sec:4-scrub.py}). Available arguments which determine which file types need to be cleaned are \texttt{abstracts} and/or \texttt{fullpdfs}.

All \textbf{abstracts} are still in \acronym{Html} format, therefore the script searches for all \texttt{.abstract} files and parses them via the BeautifulSoup library. It extracts the abstract texts to \texttt{.txt} files, keywords (if available) to \texttt{.keywords} files and category designations (if available) to \texttt{.categories} files, keeping the original \acronym{Utf-8}~encoding for all files. 
\begin{verbatim}
./4-scrub.py abstracts
\end{verbatim}
Conversion for all abstracts takes approximately 40 minutes. It is possible that there are errors caused by incomplete or empty web pages. These abstracts have to be re-downloaded manually and the script re-run.

In order to gain access to additional category designations that are not provided in abstracts or the issues' table of contents, the \textbf{full PDFs} are first converted to text files by the script. In a second step these text files are read in and the categories saved as \texttt{.full.category} files. The original \acronym{Pdf}s are already in an \acronym{Ocr}'d (optical character recognition) format, however in a rather low quality. Therefore the script also substitutes malformed categories like ``Biochenmstry'' for their correct strings.
A run of the script:
\begin{verbatim}
./4-scrub.py fullpdfs
\end{verbatim}
takes approximately 25 minutes.
One \acronym{Pdf} will produce an error: the file \texttt{1993/10295.full.pdf} apparently has not been published in a machine readable format. In this case the file \texttt{10295.full.category} needs to be created manually with the content \texttt{Biochemistry}.

\subsection{Merging and Inspecting the Available Metadata}
The last \proglang{Python} script \texttt{5-zip.py} (listing in Section~\ref{sec:5-zip.py}) handles the merging of all available additional metadata (see previous subsection) into the file \texttt{meta.csv}.
It also performs additional sanity checks to ensure that all data is complete and puts out warnings if otherwise.
The script depends upon a version of \texttt{selected.csv} in which all relevant abstracts have been tagged for download. Such a file is produced by calling \texttt{2-select.py} without the \texttt{-m} parameters (see previous Subsection~\ref{sec:selecting}).

For debugging purposes the script can be called with the parameter \texttt{all}, resulting in all lines of \texttt{selected.csv} to be evaluated and saved in the output file. If the argument \texttt{skiperrors} is supplied instead, only items with complete metadata will be returned. In both cases, a field called \texttt{warning} can be inspected in the output file \texttt{meta.csv}, which lists any inconsistencies detected by the script.

Running:
\begin{verbatim}
./5-zip.py skiperrors
\end{verbatim}
will therefore produce the file \texttt{meta.csv} with one header row and the same number of rows that were selected in \texttt{selected.csv} (given that downloads are complete). The following columns are now available:
\begin{description}
 \item[Major and minor category]: from the issue's table of content. Relevant only for the 2001~analysis of categories in Chapter~\ref{ch:analysing-pnas}.
 \item[Authors, title, volume, issue, pages]: irrelevant.
 \item[Year]: relevant.
 \item[\acronym{URL}s to \acronym{HTML} abstracts, extracts, full texts and full \acronym{PDF}s]: needed by download script.
 \item[Path to local abstract .txt]: Relevant for matching abstracts and metadata by serving as a unique identifier. See next subsection.
 \item[Duplicate, download abstract, download full \acronym{PDF}]: binary (\texttt{yes} or empty), relevant only for previous scripts.
 \item[Category in full \acronym{PDF}, major and minor categories in abstract]: Relevant for the final merged category.
 \item[Merged category]: needed for the trend analysis of the whole corpus. As can be seen in the script's source code (Section~\ref{sec:5-zip.py}), all available category metadata is imported into a new ``merged'' category. Most minor categories are left intact, apart from a few substitutions to compensate for outliers:
 \begin{center}
\begin{tabular}{ll p{6cm}}\hline
Original category & Merged category & Comments\\\hline
Botany & Plant Biology & only 1991, a few dozen in total\\
Molecular Biology & Biochemistry & only one abstract (1996)\\
Neurosciences & Neurobiology & only one abstract (1994)\\
Pharmacology & Physiology/Pharmacology & to account for category Phys./Pharm. from 1991\\
Physiology & Physiology/Pharmacology & see above\\
Plant Sciences & Plant Biology &only one abstract (1992) \\
Political Sciences & Social Sciences & only two abstracts (1999)\\\hline
 \end{tabular}
 \end{center}
An additional script (\texttt{opt-categories.py}, see Section~\ref{sec:opt-categories.py}) was used for listing the categories of each year in a \acronym{Csv} file, allowing me to compare the categories in a spreadsheet and manually produce the table above.
 \item[Keywords]: these data are not available for all abstracts, therefore useless for our purposes.
 \item[Warnings]: irrelevant when the file was produced with the \texttt{skiperrors} argument.
\end{description}

\subsection{Importing Abstracts and Metadata as a \textbf{tm} Corpus in \textsf{R}}

With the metadata complete and all abstracts saved as local text files we can now proceed to import the data in \proglang{R}. This is accomplished by executing:
\begin{verbatim}
R CMD BATCH --vanilla tm-corpus-make.R
\end{verbatim} 

The script \texttt{tm-corpus-make.R} (listing in Section~\ref{sec:tm-corpus-make.R}) reads in both metadata and abstracts, matches and merges them, and saves the resulting \pkg{tm} corpus to the file \texttt{tm-corpus-pnas-abstracts.RData}. The resulting file can then be loaded in \proglang{R} for further processing. It is worth noting that the initial 1 GiB of \acronym{Html} abstracts are converted to a compressed file of a mere 16 MiB which also includes metadata.
A more detailed description of the corpus can be found in Chapter \ref{ch:analysing-pnas}.

% Source:
% per year: http://www.pnas.org/content/by/year/1991
% per issue: http://www.pnas.org/content/vol88/issue1/ (redirected)
% really: http://www.pnas.org/content/88/1.toc (direct links, used like this in first version)
% (new script: cleanttoclinks.sh to test sed)


%This leaves us with 27,462 UTF-8 encoded text abstracts with the paper title in the first line and the abstract in one single second line, stripped of formulae, captions etc.

% Web scraping: document what happens with captions, tables, formulars, references, ligatures? 

% check for unicode content:
% grep -r β .


%PNASabstracts <- Corpus(PNASSource, readerControl = list(reader = PNASReader, language = "en_US"))
%Error in elem$content[[-1]] : attempt to select less than one element


\section{Listings}
\label{code:scrapy}

%\ti{possible further formatting options to listings: make list of listings, captions, }
% \lstlistoflistings

\lstset{ %
% language=Octave,                % choose the language of the code
basicstyle=\footnotesize\ttfamily,       % the size of the fonts that are used for the code
commentstyle=,
numbers=left,                   % where to put the line-numbers
% numberstyle=\footnotesize,      % the size of the fonts that are used for the line-numbers
% stepnumber=2,                   % the step between two line-numbers. If it's 1 each line 
%                                 % will be numbered
% numbersep=5pt,                  % how far the line-numbers are from the code
% backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
showspaces=false,               % show spaces adding particular underscores
showstringspaces=false,         % underline spaces within strings
% showtabs=false,                 % show tabs within strings adding particular underscores
% frame=single,	                % adds a frame around the code
tabsize=2,	                % sets default tabsize to 2 spaces
% captionpos=b,                   % sets the caption-position to bottom
breaklines=true,                % sets automatic line breaking
% breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
% title=\lstname,                 % show the filename of files included with \lstinputlisting;
%                                 % also try caption instead of title
% escapeinside={\%*}{*)},         % if you want to add a comment within your code
% morekeywords={*,...}            % if you want to add more keywords to the set
stringstyle=\ttfamily
}

\subsection{\texttt{settings.py}}
\label{sec:settings.py}
\lstinputlisting[language=python]{web-scraping/scrapy/settings.py}

\subsection{\texttt{items.py}}
\label{sec:items.py}
\lstinputlisting[language=python]{web-scraping/scrapy/items.py}

\subsection{\texttt{pnas\_spider.py}}
\label{sec:pnas-spider.py}
\lstinputlisting[language=python]{web-scraping/scrapy/pnas_spider.py}

\subsection{\texttt{pipelines.py}}
\label{sec:pipeline.py}
\lstinputlisting[language=python]{web-scraping/scrapy/pipelines.py}

\subsection{\texttt{1-scrape.sh}}
\label{sec:1-scrape.sh}
\lstinputlisting[language=bash]{web-scraping/1-scrape.sh}

\subsection{\texttt{2-select.py}}
\label{sec:2-select.py}
\lstinputlisting[language=python]{web-scraping/2-select.py}

\subsection{\texttt{3-get.py}}
\label{sec:3-get.py}
\lstinputlisting[language=python]{web-scraping/3-get.py}

\subsection{\texttt{4-scrub.py}}
\label{sec:4-scrub.py}
\lstinputlisting[language=python]{web-scraping/4-scrub.py}

\subsection{\texttt{5-zip.py}}
\label{sec:5-zip.py}
\lstinputlisting[language=python]{web-scraping/5-zip.py}

\subsection{\texttt{csv\_unicode.py}}
\label{sec:csv_unicode.py}
\lstinputlisting[language=python]{web-scraping/csv_unicode.py}

\subsection{\texttt{opt-categories.py}}
\label{sec:opt-categories.py}
\lstinputlisting[language=python]{web-scraping/opt-categories.py}

\subsection{\texttt{tm-corpus-make.R}}
\label{sec:tm-corpus-make.R}
\lstinputlisting[language=r]{web-scraping/tm-corpus-make.R}

% % % Keywords (if present) are left in: list items start with: *, keywords: term1_term2, therefore subsitution of _ as well as * makes sense. (is this done by tm tokenizer?)
% 
% Full: Headers like: Abstract, Conclusion etc are part of the full text.
% Figures, tables, references, footnotes, acknowledgments and related articles were omitted
% 
% Metadata: tab separated values, sorted by ``sort'' which is the same order as the files are read in, therefore easy lookup is possible.
% 
% 
% \url{http://www.pnas.org/site/misc/iforc.shtml#order}
% PHYSICAL SCIENCES: Applied Mathematics, Applied Physical Sciences, Astronomy, Chemistry, Computer Sciences, Engineering, Environmental Sciences, Geology, Geophysics, Mathematics, Physics, Statistics, and Sustainability Science.
% 
% SOCIAL SCIENCES: Anthropology, Economic Sciences, Environmental Sciences, Political Sciences, Psychological and Cognitive Sciences, Social Sciences, and Sustainability Science.
% 
% BIOLOGICAL SCIENCES: Agricultural Sciences, Anthropology, Applied Biological Sciences, Biochemistry, Biophysics and Computational Biology, Cell Biology, Developmental Biology, Ecology, Environmental Sciences, Evolution, Genetics, Immunology, Medical Sciences, Microbiology, Neuroscience, Pharmacology, Physiology, Plant Biology, Population Biology, Psychological and Cognitive Sciences, Sustainability Science, and Systems Biology
% 
% % \subsection{PDF keywords}
% 1999y, 1999-05y, 1999-08mixed, 1999-12n, 2000n (online PDFs?)

% \subsection{Webpage headings}
% 1915-1980s: mostly present.
% Mostly header: ``Research Article'', Colloquium Papers?
% Sometimes:
% <h2 id="PhysicalSciences">, <h3 id="PhysicalSciencesChemistry">
% 